{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Announcements\n",
    "\n",
    "**Assignment 5**<br>\n",
    "\n",
    "**Project 2**<br>\n",
    "Due: December 1, 2021, class time.\n",
    "\n",
    "**Practicing makes much difference.** \n",
    "\n",
    "**Assignments and Projects**<br>\n",
    "For assistance in your assignment or project, if you are close to the deadline (e.g., one day earlier), it may be harder to find help.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.181200Z",
     "start_time": "2021-11-10T21:26:59.176733Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ack: This material was adapted for education purposes from the following sources:\n",
    "# https://conferences.oreilly.com/oscon/oscon-or-2019/public/schedule/detail/76237.html\n",
    "# https://github.com/fastai/course-nlp\n",
    "# https://www.kaggle.com/kitakoj18/exploring-wine-descriptions-with-nlp-and-kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. What is NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## What can you do with NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "NLP is a broad field, encompassing a variety of tasks, including:\n",
    "\n",
    "- Part-of-speech tagging: identify if each word is a noun, verb, adjective, etc.)\n",
    "- Named entity recognition NER): identify person names, organizations, locations, medical codes, time expressions, quantities, monetary values, etc)\n",
    "- Question answering\n",
    "- Speech recognition\n",
    "- Text-to-speech and Speech-to-text\n",
    "- Topic modeling\n",
    "- Sentiment classification\n",
    "- Language modeling\n",
    "- Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NLP Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Regex (example: find all phone numbers: 123-456-7890, (123) 456-7890, etc.)\n",
    "- Tokenization: splitting your text into meaningful units (has a different meaning in security)\n",
    "- Word embeddings\n",
    "- Linear algebra/matrix decomposition\n",
    "- Neural nets\n",
    "- Hidden Markov Models\n",
    "- Parse trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Python Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- [nltk](https://www.nltk.org/): first released in 2001, very broad NLP library\n",
    "- [spaCy](https://spacy.io/): creates parse trees, excellent tokenizer, opinionated\n",
    "- [gensim](https://radimrehurek.com/gensim/): topic modeling and similarity detection\n",
    "- [Stanford CoreNLP](https://stanfordnlp.github.io/CoreNLP/): Code in Java, but can be used with a Python wrapper. \n",
    "\n",
    "specialized tools:\n",
    "- [PyText](https://pytext-pytext.readthedocs-hosted.com/en/latest/)\n",
    "- [fastText](https://fasttext.cc/) has library of embeddings\n",
    "\n",
    "general ML/DL libraries with text features:\n",
    "- [sklearn](https://scikit-learn.org/stable/): general purpose Python ML library\n",
    "- [fastai](https://docs.fast.ai/): fast & accurate neural nets using modern best practices, on top of PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If not already installed:\n",
    "\n",
    "Use pip to install the following packages: ```nltk, lxml, requests, re, wikipedia, wordcloud```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.209366Z",
     "start_time": "2021-11-10T21:26:59.194396Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.233915Z",
     "start_time": "2021-11-10T21:26:59.215398Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Additional Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- [Data source](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html): Newsgroups are discussion groups on Usenet, which was popular in the 80s and 90s before the web really took off.  This dataset includes 18,000 newsgroups posts with 20 topics.\n",
    "- [Chris Manning's book chapter](https://nlp.stanford.edu/IR-book/pdf/18lsi.pdf) on matrix factorization and LSI \n",
    "- Scikit learn [truncated SVD LSI details](http://scikit-learn.org/stable/modules/decomposition.html#lsa)\n",
    "\n",
    "### Other Tutorials\n",
    "- [Scikit-Learn: Out-of-core classification of text documents](http://scikit-learn.org/stable/auto_examples/applications/plot_out_of_core_classification.html): uses [Reuters-21578](https://archive.ics.uci.edu/ml/datasets/reuters-21578+text+categorization+collection) dataset (Reuters articles labeled with ~100 categories), HashingVectorizer\n",
    "- [Text Analysis with Topic Models for the Humanities and Social Sciences](https://de.dariah.eu/tatom/index.html): uses [British and French Literature dataset](https://de.dariah.eu/tatom/datasets.html) of Jane Austen, Charlotte Bronte, Victor Hugo, and more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NLTK\n",
    "Natural Language Toolkit is one of the most popular collection of libraries and programs to do NLP. You can find more about it here: http://www.nltk.org/book/ch00.html. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.253227Z",
     "start_time": "2021-11-10T21:26:59.237102Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sagar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to /Users/sagar/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "NLTK comes pre-loaded with texts from the Project Gutenberg archive that you can use. It also has a collection of informal text from discussion forums, conversations, chat sessions, movie scripts, etc. NLTK has corpora in other languages as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.398470Z",
     "start_time": "2021-11-10T21:26:59.261777Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austen-emma.txt [Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a ...\n",
      "austen-persuasion.txt [Persuasion by Jane Austen 1818]\n",
      "\n",
      "\n",
      "Chapter 1\n",
      "\n",
      "\n",
      "Sir Walter Elliot, of Kellynch Hall, in Somersetshire ...\n",
      "austen-sense.txt [Sense and Sensibility by Jane Austen 1811]\n",
      "\n",
      "CHAPTER 1\n",
      "\n",
      "\n",
      "The family of Dashwood had long been settle ...\n",
      "bible-kjv.txt [The King James Bible]\n",
      "\n",
      "The Old Testament of the King James Bible\n",
      "\n",
      "The First Book of Moses:  Called  ...\n",
      "blake-poems.txt [Poems by William Blake 1789]\n",
      "\n",
      " \n",
      "SONGS OF INNOCENCE AND OF EXPERIENCE\n",
      "and THE BOOK of THEL\n",
      "\n",
      "\n",
      " SONGS  ...\n",
      "bryant-stories.txt [Stories to Tell to Children by Sara Cone Bryant 1918] \r\n",
      "\r\n",
      "\r\n",
      "TWO LITTLE RIDDLES IN RHYME\r\n",
      "\r\n",
      "\r\n",
      "     T ...\n",
      "burgess-busterbrown.txt [The Adventures of Buster Bear by Thornton W. Burgess 1920]\r\n",
      "\r\n",
      "I\r\n",
      "\r\n",
      "BUSTER BEAR GOES FISHING\r\n",
      "\r\n",
      "\r\n",
      "Bu ...\n",
      "carroll-alice.txt [Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was ...\n",
      "chesterton-ball.txt [The Ball and The Cross by G.K. Chesterton 1909]\n",
      "\n",
      "\n",
      "I. A DISCUSSION SOMEWHAT IN THE AIR\n",
      "\n",
      "The flying s ...\n",
      "chesterton-brown.txt [The Wisdom of Father Brown by G. K. Chesterton 1914]\n",
      "\n",
      "\n",
      "I. The Absence of Mr Glass\n",
      "\n",
      "\n",
      "THE consulting- ...\n",
      "chesterton-thursday.txt [The Man Who Was Thursday by G. K. Chesterton 1908]\n",
      "\n",
      "To Edmund Clerihew Bentley\n",
      "\n",
      "A cloud was on the  ...\n",
      "edgeworth-parents.txt [The Parent's Assistant, by Maria Edgeworth]\r\n",
      "\r\n",
      "\r\n",
      "THE ORPHANS.\r\n",
      "\r\n",
      "Near the ruins of the castle of Ro ...\n",
      "melville-moby_dick.txt [Moby Dick by Herman Melville 1851]\r\n",
      "\r\n",
      "\r\n",
      "ETYMOLOGY.\r\n",
      "\r\n",
      "(Supplied by a Late Consumptive Usher to a Gr ...\n",
      "milton-paradise.txt [Paradise Lost by John Milton 1667] \n",
      " \n",
      " \n",
      "Book I \n",
      " \n",
      " \n",
      "Of Man's first disobedience, and the fruit \n",
      "Of  ...\n",
      "shakespeare-caesar.txt [The Tragedie of Julius Caesar by William Shakespeare 1599]\n",
      "\n",
      "\n",
      "Actus Primus. Scoena Prima.\n",
      "\n",
      "Enter Fla ...\n",
      "shakespeare-hamlet.txt [The Tragedie of Hamlet by William Shakespeare 1599]\n",
      "\n",
      "\n",
      "Actus Primus. Scoena Prima.\n",
      "\n",
      "Enter Barnardo a ...\n",
      "shakespeare-macbeth.txt [The Tragedie of Macbeth by William Shakespeare 1603]\n",
      "\n",
      "\n",
      "Actus Primus. Scoena Prima.\n",
      "\n",
      "Thunder and Lig ...\n",
      "whitman-leaves.txt [Leaves of Grass by Walt Whitman 1855]\n",
      "\n",
      "\n",
      "Come, said my soul,\n",
      "Such verses for my Body let us write, ( ...\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# Printing the first 100 characters of each of the files\n",
    "for fileid in gutenberg.fileids():\n",
    "    print(fileid, gutenberg.raw(fileid)[:100], '...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Getting the data\n",
    "\n",
    "Data can come from a variety of sources in different formats. Natural language can be in the form of text or speech. In this class, we will be focusing on text-based processing as opposed to speech recognition and synthesis. <br>\n",
    "\n",
    "Textual data can be stored in databases, dataframes, text files, webpages, etc. A list of text datasets can be found here: https://github.com/niderhoff/nlp-datasets. Let's create a list with a few sentences that will serve as the sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.430016Z",
     "start_time": "2021-11-10T21:26:59.422454Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today is 10th of November. We are at GA State in Atlanta. Currently we are in a MSA 8010 class.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data = [\"Today is 10th of November. We are at GA State in Atlanta. Currently we are \" \\\n",
    "                \"in a MSA 8010 class.\"]\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sentence segmentation\n",
    "A paragraph is nothing but a collection of sentences. Also called sentence tokenization or sentence boundary disambiguation, this process breaks up sentences by deciding where a sentence starts and ends. Challenges include recognizing ambiguous puncutation marks. For example, `.` can be used for a decimal point, an ellipsis or a period. \n",
    "\n",
    "We can use ```sent_tokenize``` from NLTK ```nltk.tokenize``` to get sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.437966Z",
     "start_time": "2021-11-10T21:26:59.432948Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def get_sent_tokens(data):\n",
    "    \"\"\"Sentence tokenization\"\"\"\n",
    "    sentences = []\n",
    "    for sent in data:\n",
    "        sentences.extend(sent_tokenize(sent))\n",
    "    print('Sentence tokens:', sentences)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.449168Z",
     "start_time": "2021-11-10T21:26:59.441294Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence tokens: ['Today is 10th of November.', 'We are at GA State in Atlanta.', 'Currently we are in a MSA 8010 class.']\n"
     ]
    }
   ],
   "source": [
    "sample_sentences = get_sent_tokens(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word tokenization\n",
    "A sentence is a collection of words. Word tokenization is similar to sentence tokenization, but works on words. Let's use ```word_tokenize``` from ```nltk.tokenize``` to get the words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.459294Z",
     "start_time": "2021-11-10T21:26:59.451899Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def get_word_tokens(sentences):\n",
    "    '''Word tokenization'''\n",
    "    words = []\n",
    "    for sent in sentences:\n",
    "        words.extend(word_tokenize(sent))\n",
    "    print('Word tokens:', words)\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.467561Z",
     "start_time": "2021-11-10T21:26:59.461734Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tokens: ['Today', 'is', '10th', 'of', 'November', '.', 'We', 'are', 'at', 'GA', 'State', 'in', 'Atlanta', '.', 'Currently', 'we', 'are', 'in', 'a', 'MSA', '8010', 'class', '.']\n"
     ]
    }
   ],
   "source": [
    "sample_words = get_word_tokens(sample_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Frequency distribution\n",
    "Calculates the frequency distribution for each word in the data. Use ```nltk.probability``` from ```FreqDist``` and ```matplotlib```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.479014Z",
     "start_time": "2021-11-10T21:26:59.473195Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from nltk.probability import FreqDist\n",
    "matplotlib.use('TkAgg') \n",
    "\n",
    "def plot_freq_dist(words, num_words = 20):\n",
    "    '''Frequency distribution'''\n",
    "    fdist = FreqDist(words)\n",
    "    fdist.plot(num_words, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.685144Z",
     "start_time": "2021-11-10T21:26:59.482002Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plot_freq_dist(sample_words, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cleaning the data\n",
    "\n",
    "Real world data is often messy and needs cleaning. You can perform preprocessing including:\n",
    "- Removing special characters and numbers - These are usually not important when trying to derive the semantics\n",
    "- Removing stopwords - A special category of words that don't have any significance on their own and are often used as filler words or to ensure correct grammer. Eg. the, and, but, of, is, or, those, her, \n",
    "- Removing HTML tags - Raw data from webpages can often be laden with HTML tags. Use a library like `BeautifulSoup` to process and remove the tags.\n",
    "- Standardizing words - This aims to consolidate different versions of the same version Eg. SMS/Twitter language, slang, misspellings \n",
    "- Converting to lower case - To ensure uniformity across all words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "From [Intro to Information Retrieval](https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html):\n",
    "\n",
    "*Some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop words.*\n",
    "\n",
    "*The general trend in IR systems over time has been from standard use of quite large stop lists (200-300 terms) to very small stop lists (7-12 terms) to no stop list whatsoever. Web search engines generally do not use stop lists.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.696058Z",
     "start_time": "2021-11-10T21:26:59.687369Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amoungst']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import _stop_words\n",
    "\n",
    "sorted(list(_stop_words.ENGLISH_STOP_WORDS))[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There is no single universal list of stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.713447Z",
     "start_time": "2021-11-10T21:26:59.699116Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/sagar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def get_clean_sentences(sentences, remove_digits=False):\n",
    "    '''Cleaning sentences by removing special characters and optionally digits'''\n",
    "    clean_sentences = []\n",
    "    for sent in sentences:\n",
    "        pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]' \n",
    "        clean_text = re.sub(pattern, '', sent)\n",
    "        clean_text = clean_text.lower()\n",
    "        clean_sentences.append(clean_text)\n",
    "    print('Clean sentences:', clean_sentences)\n",
    "    return clean_sentences\n",
    "\n",
    "def filter_stopwords(words):\n",
    "    '''Removing stopwords from given words'''\n",
    "    filtered_words = [w for w in words if w not in stop_words]\n",
    "    print('Filtered words:', filtered_words)\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.722377Z",
     "start_time": "2021-11-10T21:26:59.716103Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean sentences: ['today is th of november', 'we are at ga state in atlanta', 'currently we are in a msa  class']\n",
      "Word tokens: ['today', 'is', 'th', 'of', 'november', 'we', 'are', 'at', 'ga', 'state', 'in', 'atlanta', 'currently', 'we', 'are', 'in', 'a', 'msa', 'class']\n",
      "Filtered words: ['today', 'th', 'november', 'ga', 'state', 'atlanta', 'currently', 'msa', 'class']\n"
     ]
    }
   ],
   "source": [
    "sample_sentences = get_clean_sentences(sample_sentences, remove_digits = True)\n",
    "sample_words = get_word_tokens(sample_sentences)\n",
    "sample_words = filter_stopwords(sample_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "After cleaning the text and using tokenization, we are left with words. Words have certain properties which we'll be exploring in the next few sections. These characteristics can often be used as features for a Machine Learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## POS tagging\n",
    "\n",
    "The English language is formed of different parts of speech (POS) like nouns, verbs, pronouns, adjectives, etc. POS tagging analyzes the words in a sentences and associates it with a POS tag depending on the way it is used. Also called grammatical tagging or word-category disambiguation. Use ```nltk.pos_tag``` for the process. There are different types of tagsets used with the most common being the Penn Treebank tagset and the Universal tagset. \n",
    "\n",
    "![Penn POS tags](https://slideplayer.com/slide/6855236/23/images/11/Penn+TreeBank+POS+Tag+set.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.734282Z",
     "start_time": "2021-11-10T21:26:59.726173Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/sagar/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def get_pos_tags(words):\n",
    "    '''Get the part of speech (POS) tags for the words'''\n",
    "    tags=[]\n",
    "    for word in words:\n",
    "        tags.append(nltk.pos_tag([word]))\n",
    "#     print(tags)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.756946Z",
     "start_time": "2021-11-10T21:26:59.736794Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('today', 'NN')],\n",
       " [('th', 'NN')],\n",
       " [('november', 'NN')],\n",
       " [('ga', 'NN')],\n",
       " [('state', 'NN')],\n",
       " [('atlanta', 'NN')],\n",
       " [('currently', 'RB')],\n",
       " [('msa', 'NN')],\n",
       " [('class', 'NN')]]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_tags = get_pos_tags(sample_words)\n",
    "sample_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text processing\n",
    "Text processing approaches like stemming and lemmatization help in reducing inflectional forms of words. \n",
    "### Dictionary and thesaurus\n",
    "WordNet is a lexical database that also has relationships between different words. You can use synsets to find definitions, synonyms and antonyms for words. You can also find hyponyms and hypernyms using the same process. Hypernym is a generalized concept like 'programming language' whereas hyponym is a specific concept like 'Python' or 'Java'.\n",
    "\n",
    "![Hypernym and hyponym](https://upload.wikimedia.org/wikipedia/en/thumb/1/1f/Hyponymsandhypernyms.jpg/300px-Hyponymsandhypernyms.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.767039Z",
     "start_time": "2021-11-10T21:26:59.759959Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/sagar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "def get_wordnet_properties(words):\n",
    "    '''Returns definition, synonyms and antonyms of words'''\n",
    "    for word in words:\n",
    "        synonyms = []\n",
    "        antonyms = []\n",
    "#         hyponyms = []\n",
    "#         hypernyms = []\n",
    "        definitions = []\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for lm in syn.lemmas():\n",
    "                synonyms.append(lm.name())\n",
    "                if lm.antonyms(): \n",
    "                    antonyms.append(lm.antonyms()[0].name())\n",
    "#             hyponyms.append(syn.hyponyms())\n",
    "#             hypernyms.append(syn.hypernyms())\n",
    "#             definitions.append(syn.definition())\n",
    "            \n",
    "        print(word)\n",
    "        print('Synonyms:', synonyms, '\\nAntonyms:', antonyms, '\\n')\n",
    "#         print('Definition:', definitions, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Have you watched the series 'Friends'? Do you remember the [episode](https://youtu.be/B1tOqZUNebs?t=100) where Joey has to write a letter of recommendation for Monica and Chandler for the adoption agency? He uses a thesaurus to make himself sound smarter in the letter! Let's see if we get the same results:\n",
    "\n",
    "'They are warm, nice people with big hearts' -> 'They are humid, prepossessing Homo Sapiens with full-sized aortic pumps'\n",
    "\n",
    "![Joey Friends](https://media.giphy.com/media/VEsfbW0pBu145PPhOi/giphy.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.849546Z",
     "start_time": "2021-11-10T21:26:59.769583Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wordnet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/j2/52n1_kys0455pytggvwjfss00000gn/T/ipykernel_42602/2525791567.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mjoey_dialogue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'they'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'are'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'warm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'people'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'with'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'big'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hearts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_wordnet_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoey_dialogue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/j2/52n1_kys0455pytggvwjfss00000gn/T/ipykernel_42602/3185878726.py\u001b[0m in \u001b[0;36mget_wordnet_properties\u001b[0;34m(words)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#         hypernyms = []\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mdefinitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msyn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msyn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0msynonyms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wordnet' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "joey_dialogue = ['they', 'are', 'warm', 'nice', 'people', 'with', 'big', 'hearts']\n",
    "get_wordnet_properties(joey_dialogue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word Sense Disambiguation\n",
    "\n",
    "These synsets are also used for disambiguation, particularly Word Sense Disambiguation using Lesk Algorithm. See: http://www.nltk.org/howto/wsd.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.860237Z",
     "start_time": "2021-11-10T21:26:59.860212Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.wsd import lesk\n",
    "sent = ['I', 'went', 'to', 'the', 'bank', 'to', 'deposit', 'money', '.']\n",
    "print(lesk(sent, 'bank', 'n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.862524Z",
     "start_time": "2021-11-10T21:26:59.862476Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sent = ['I', 'was', 'sitting', 'by', 'the', 'bank', '.']\n",
    "print(lesk(sent, 'bank', 'n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.864975Z",
     "start_time": "2021-11-10T21:26:59.864948Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sent = ['I', 'was', 'waiting', 'at', 'the', 'blood', 'bank', '.']\n",
    "print(lesk(sent, 'bank', 'n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "from [Information Retrieval](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) textbook:\n",
    "\n",
    "Are the below words the same?\n",
    "\n",
    "*organize, organizes, and organizing*\n",
    "\n",
    "*democracy, democratic, and democratization*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Stemming and Lemmatization both generate the root form of the words. \n",
    "\n",
    "Lemmatization uses the rules about a language.  The resulting tokens are all actual words\n",
    "\n",
    "\"Stemming is the poor-man’s lemmatization.\" (Noah Smith, 2011) Stemming is a crude heuristic that chops the ends off of words.  The resulting tokens may not be actual words. Stemming is faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.867776Z",
     "start_time": "2021-11-10T21:26:59.867750Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Stemming\n",
    "Stemming tries to cut off at the ends of the words in the hope of deriving the base form. Stems aren't always real words. Use ```PorterStemmer``` from ```ntlk.stem```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.869737Z",
     "start_time": "2021-11-10T21:26:59.869673Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import stem\n",
    "\n",
    "wnl = stem.WordNetLemmatizer()\n",
    "porter = stem.porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.872023Z",
     "start_time": "2021-11-10T21:26:59.871999Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "[porter.stem(word) for word in word_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lemmatization\n",
    "Lemmatization groups different inflected forms of a words so they can be mapped to the same base. Lemmas are real words. More complex than stemming, context of words is also analyzed. Uses WordNet which is a lexical English database. \n",
    "Use ```WordNetLemmatizer``` from ```nltk.stem``` and provide it the POS tag along with the word. NLTK’s POS tags are in a format different from to that of wordnet lemmatizer, so a mapping is needed. https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.873674Z",
     "start_time": "2021-11-10T21:26:59.873649Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "word_list = ['feet', 'foot', 'foots', 'footing']\n",
    "\n",
    "[wnl.lemmatize(word) for word in word_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Stemming and lemmatization are language dependent.  Languages with more complex morphologies may show bigger benefits.  For example, Sanskrit has a very [large number of verb forms](https://en.wikipedia.org/wiki/Sanskrit_verbs). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "These were long considered standard techniques, but they can often **hurt** your performance **if using deep learning**. Stemming, lemmatization, and removing stop words all involve throwing away information.\n",
    "\n",
    "However, they can still be useful when working with simpler models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distances \n",
    "You can calculate distances between words. There are a variety of distance metrics available: https://en.wikipedia.org/wiki/String_metric. The most common ones are Levenshtein, Cosine distances and Jaccard similarity. Applications include spell checking, correction for OCRs and Machine Translation. For an implementation of a spell checker, see here: https://norvig.com/spell-correct.html\n",
    "\n",
    "![Edit distance](https://i.stack.imgur.com/5Pjr7.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Named Entity Recognition (NER) \n",
    "\n",
    "Also known as entity chunking or extraction, is a sub-process of information extraction. This involves identifies and classifies named entities mentions into sub-categories like person name, organization, location, time, etc.  In other words, Named Entity Recognition (NER) labels sequences of words in a text which are the names of things, such as person and company names, or gene and protein names. \n",
    "\n",
    "Some of the most popular NER models are here: https://towardsdatascience.com/a-review-of-named-entity-recognition-ner-using-automatic-summarization-of-resumes-5248a75de175. <br>\n",
    "\n",
    "Example use-cases include customer support, search engine, news classification. Another emerging application is for redacting personally identifiable information (PII). A great demo of NER in action is here: https://explosion.ai/demos/displacy-ent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bag of words\n",
    "Bag of words is an approach for text feature extraction. Just imagine a bag of popcorn, \n",
    "and each popcorn kernel represents a word that is present in the text. Each sentence can be represented as a vector\n",
    "of all the words present in a vocabulary. If a word is present in the sentence, it is 1, otherwise 0.\n",
    "\n",
    "![Bag of words](https://cdn-images-1.medium.com/max/1600/1*zMdHVQQ7HYv_mMZ5Ne-2yQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TF-IDF\n",
    "Term-frequency inverse document frequency assigns scores to words inside a document. Commonly occuring words in all documents would have less weightage.\n",
    "![TF IDF](http://www.bloter.net/wp-content/uploads/2016/09/td-idf-graphic.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.875795Z",
     "start_time": "2021-11-10T21:26:59.875731Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_bag_of_words(sentences):\n",
    "    ''''''\n",
    "    vectorizer = CountVectorizer()\n",
    "    print(vectorizer.fit_transform(sentences).todense())\n",
    "    print(vectorizer.vocabulary_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.877908Z",
     "start_time": "2021-11-10T21:26:59.877888Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "get_bag_of_words(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word embeddings - Word2Vec\n",
    "Vector space model - represent words and sentences as vectors to get semantic relationships. A really good tutorial for Word2Vec is here: https://www.kaggle.com/alvations/word2vec-embedding-using-gensim-and-nltk\n",
    "\n",
    "![Word2Vec](http://www.flyml.net/wp-content/uploads/2016/11/w2v-3-samples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine Learning using Natural Language Processing\n",
    "Machine Learning includes two approaches: supervised and unsupervised. Supervised learning works on data that already has labels i.e. they provide supervision to the model. Eg. Classification, Regression. Unsupervised learning is to find out the inherent structure present in the data and there are no labels i.e. no supervision. Eg. Clustering.\n",
    "\n",
    "A lot of these text properties can be used as features for Machine Learning systems. One specific case is text classification. A more detailed resource is here: https://www.nltk.org/book/ch06.html\n",
    "\n",
    "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics. More details here: https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:27:25.426462Z",
     "start_time": "2021-11-10T21:27:25.053135Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes present: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "Number of classes present: 20\n",
      "Number of data points: 18846\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "news = fetch_20newsgroups(subset='all')\n",
    "print('Classes present:', news.target_names)\n",
    "print('Number of classes present:', len(news.target_names))\n",
    "print('Number of data points:', len(news.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:27:36.936681Z",
     "start_time": "2021-11-10T21:27:36.931575Z"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rec.sport.hockey]:\t\t \"From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu> ...\"\n",
      "[comp.sys.ibm.pc.hardware]:\t\t \"From: mblawson@midway.ecn.uoknor.edu (Matthew B Lawson) ...\"\n",
      "[talk.politics.mideast]:\t\t \"From: hilmi-er@dsv.su.se (Hilmi Eren) ...\"\n",
      "[comp.sys.ibm.pc.hardware]:\t\t \"From: guyd@austin.ibm.com (Guy Dawson) ...\"\n",
      "[comp.sys.mac.hardware]:\t\t \"From: Alexander Samuel McDiarmid <am2o+@andrew.cmu.edu> ...\"\n",
      "[sci.electronics]:\t\t \"From: tell@cs.unc.edu (Stephen Tell) ...\"\n",
      "[comp.sys.mac.hardware]:\t\t \"From: lpa8921@tamuts.tamu.edu (Louis Paul Adams) ...\"\n",
      "[rec.sport.hockey]:\t\t \"From: dchhabra@stpl.ists.ca (Deepak Chhabra) ...\"\n",
      "[rec.sport.hockey]:\t\t \"From: dchhabra@stpl.ists.ca (Deepak Chhabra) ...\"\n",
      "[talk.religion.misc]:\t\t \"From: arromdee@jyusenkyou.cs.jhu.edu (Ken Arromdee) ...\"\n"
     ]
    }
   ],
   "source": [
    "# Printing the first few characters for each category\n",
    "\n",
    "for text, num_label in zip(news.data[:10], news.target[:10]):\n",
    "    print('[%s]:\\t\\t \"%s ...\"' % (news.target_names[num_label], text[:100].split('\\n')[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dividing into training and test data sets\n",
    "Think of it as learning in class (training) and then taking an exam (testing) to evaluate your performance. The testing is done on unseen data to know the actual abilities of the classifier i.e. preventing memorization or rote-learning. Test data set is usually 20-25% of the data set. You can also use cross-validation to ensure robustness of classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:27:38.041573Z",
     "start_time": "2021-11-10T21:27:38.035491Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "def train(classifier, X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=33)\n",
    " \n",
    "    classifier.fit(X_train, y_train)\n",
    "    print(\"Accuracy:\", classifier.score(X_test, y_test))\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes\n",
    "Probabilistic classifier based on Bayes theorem. Assumes independence among the features. Details here: https://en.wikipedia.org/wiki/Naive_Bayes_classifier\n",
    "\n",
    "We'll be using Pipeline to apply transformations sequentially: https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "\n",
    "Adapted from: https://nlpforhackers.io/text-classification/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature: TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:27:39.159207Z",
     "start_time": "2021-11-10T21:27:39.155203Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    " \n",
    "model_1 = Pipeline([('vectorizer', TfidfVectorizer()),('classifier', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:27:44.845485Z",
     "start_time": "2021-11-10T21:27:39.972179Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for TF-IDF as feature:\n",
      "Accuracy: 0.8463497453310697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer', TfidfVectorizer()),\n",
       "                ('classifier', MultinomialNB())])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Results for TF-IDF as feature:')\n",
    "train(model_1, news.data, news.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature: TF i.e. Removing IDF from TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:27:45.854738Z",
     "start_time": "2021-11-10T21:27:45.849574Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model_2 = Pipeline([('vectorizer', TfidfVectorizer(use_idf=False)),\n",
    "                    ('classifier', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:27:51.363545Z",
     "start_time": "2021-11-10T21:27:46.833909Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for TF as feature, removing IDF\n",
      "Accuracy: 0.756578947368421\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer', TfidfVectorizer(use_idf=False)),\n",
       "                ('classifier', MultinomialNB())])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Results for TF as feature, removing IDF')\n",
    "train(model_2, news.data, news.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Insight:** So, IDF does make a huge difference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature: TF-IDF + stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:27:51.709146Z",
     "start_time": "2021-11-10T21:27:51.699353Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model_3 = Pipeline([('vectorizer', TfidfVectorizer(stop_words=stopwords.words('english'))),\n",
    "                    ('classifier', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:27:56.555760Z",
     "start_time": "2021-11-10T21:27:52.031423Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for TF-IDF as feature, using stopword removal:\n",
      "Accuracy: 0.8777589134125636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
       "                                             'our', 'ours', 'ourselves', 'you',\n",
       "                                             \"you're\", \"you've\", \"you'll\",\n",
       "                                             \"you'd\", 'your', 'yours',\n",
       "                                             'yourself', 'yourselves', 'he',\n",
       "                                             'him', 'his', 'himself', 'she',\n",
       "                                             \"she's\", 'her', 'hers', 'herself',\n",
       "                                             'it', \"it's\", 'its', 'itself', ...])),\n",
       "                ('classifier', MultinomialNB())])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Results for TF-IDF as feature, using stopword removal:')\n",
    "train(model_3, news.data, news.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature: TF-IDF + stopwords removal + ignoring words with frequency < 5\n",
    "Trying simple things may work too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:29:12.394485Z",
     "start_time": "2021-11-10T21:29:12.388042Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model_4 = Pipeline([('vectorizer', TfidfVectorizer(stop_words=stopwords.words('english'), min_df=5)),\n",
    "                    ('classifier', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:29:17.487704Z",
     "start_time": "2021-11-10T21:29:13.212043Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for TF-IDF + stopwords removal + ignoring words with frequency < 5:\n",
      "Accuracy: 0.8820033955857386\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 TfidfVectorizer(min_df=5,\n",
       "                                 stop_words=['i', 'me', 'my', 'myself', 'we',\n",
       "                                             'our', 'ours', 'ourselves', 'you',\n",
       "                                             \"you're\", \"you've\", \"you'll\",\n",
       "                                             \"you'd\", 'your', 'yours',\n",
       "                                             'yourself', 'yourselves', 'he',\n",
       "                                             'him', 'his', 'himself', 'she',\n",
       "                                             \"she's\", 'her', 'hers', 'herself',\n",
       "                                             'it', \"it's\", 'its', 'itself', ...])),\n",
       "                ('classifier', MultinomialNB())])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Results for TF-IDF + stopwords removal + ignoring words with frequency < 5:')\n",
    "train(model_4, news.data, news.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature: TF-IDF + stopwords removal + ignoring words with frequency < 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:29:17.863764Z",
     "start_time": "2021-11-10T21:29:17.856918Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model_5 = Pipeline([('vectorizer', TfidfVectorizer(stop_words=stopwords.words('english'), min_df=10)),\n",
    "                    ('classifier', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:29:23.026790Z",
     "start_time": "2021-11-10T21:29:18.863153Z"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for TF-IDF + stopwords removal + ignoring words with frequency < 10:\n",
      "Accuracy: 0.8745755517826825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 TfidfVectorizer(min_df=10,\n",
       "                                 stop_words=['i', 'me', 'my', 'myself', 'we',\n",
       "                                             'our', 'ours', 'ourselves', 'you',\n",
       "                                             \"you're\", \"you've\", \"you'll\",\n",
       "                                             \"you'd\", 'your', 'yours',\n",
       "                                             'yourself', 'yourselves', 'he',\n",
       "                                             'him', 'his', 'himself', 'she',\n",
       "                                             \"she's\", 'her', 'hers', 'herself',\n",
       "                                             'it', \"it's\", 'its', 'itself', ...])),\n",
       "                ('classifier', MultinomialNB())])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Results for TF-IDF + stopwords removal + ignoring words with frequency < 10:')\n",
    "train(model_5, news.data, news.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Make sure to not go overboard with simple steps!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature: TF-IDF + stopwords removal + ignoring words with frequency < 5 + tuning hyperparameter alpha\n",
    "Alpha is a hyperparameter for smoothing in Multinomial NB that controls the model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:30:52.710150Z",
     "start_time": "2021-11-10T21:30:52.704017Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "model_6 = Pipeline([('vectorizer', TfidfVectorizer(min_df = 5,\n",
    "                     stop_words=stopwords.words('english') + list(string.punctuation))),\n",
    "                   ('classifier', MultinomialNB(alpha=0.05))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.911772Z",
     "start_time": "2021-11-10T21:26:59.911754Z"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('Results for TF-IDF + stopwords removal + ignoring words with frequency < 5 + tuning hyperparameter alpha:')\n",
    "train(model_6, news.data, news.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature: TF-IDF + stopwords removal + ignoring words with frequency < 5 + tuning hyperparameter alpha + stemming\n",
    "Let's check if stemming the words makes any difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.913469Z",
     "start_time": "2021-11-10T21:26:59.913445Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    " \n",
    "def stem_tokenizer(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in word_tokenize(text)]\n",
    " \n",
    "model_7 = Pipeline([('vectorizer', TfidfVectorizer(tokenizer=stem_tokenizer, min_df = 5,\n",
    "                     stop_words=stopwords.words('english') + list(string.punctuation))),\n",
    "                   ('classifier', MultinomialNB(alpha=0.05))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.914980Z",
     "start_time": "2021-11-10T21:26:59.914934Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('Results for TF-IDF + stopwords removal + ignoring words with frequency < 5 + \\\n",
    "       tuning hyperparameter alpha + stemming:')\n",
    "train(model_7, news.data, news.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Feel free to experiment with other features and see how well the classifier performs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sentiment analysis\n",
    "\n",
    "NLTK's VADER algorithm is used to detect polarity of words and establish the overall sentiment (compound score) for sentences. We're using a small sample of IMDB review titles for Captain Marvel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:31:22.518691Z",
     "start_time": "2021-11-10T21:31:22.461406Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/sagar/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "\n",
    "def get_sentiment(data):\n",
    "    '''Get sentiment of sentences using VADER algorithm'''\n",
    "    scorer = SentimentIntensityAnalyzer()\n",
    "    for sentence in reviews:\n",
    "        print(sentence)\n",
    "        ss = scorer.polarity_scores(sentence)\n",
    "        for k in ss:\n",
    "            print('{0}: {1}, ' .format(k, ss[k]), end='')\n",
    "        print()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:31:25.257940Z",
     "start_time": "2021-11-10T21:31:25.237751Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've watched this movie for the record, cause I had no particular expectations for it.\n",
      "neg: 0.145, neu: 0.855, pos: 0.0, compound: -0.296, \n",
      "It's fine.\n",
      "neg: 0.0, neu: 0.357, pos: 0.643, compound: 0.2023, \n",
      "Not bad, not great either.\n",
      "neg: 0.0, neu: 0.351, pos: 0.649, compound: 0.6754, \n",
      "Captain Marvel is another fine Marvel adventure.\n",
      "neg: 0.0, neu: 0.236, pos: 0.764, compound: 0.8271, \n",
      "Captivating story and great visuals.\n",
      "neg: 0.0, neu: 0.494, pos: 0.506, compound: 0.6249, \n",
      "Confusing boring childish.\n",
      "neg: 1.0, neu: 0.0, pos: 0.0, compound: -0.6597, \n",
      "Strong Hero. Weak Film.\n",
      "neg: 0.269, neu: 0.093, pos: 0.639, compound: 0.6124, \n"
     ]
    }
   ],
   "source": [
    "reviews = [\"I've watched this movie for the record, cause I had no particular expectations for it.\",\n",
    "          \"It's fine.\",\n",
    "          \"Not bad, not great either.\",\n",
    "          \"Captain Marvel is another fine Marvel adventure.\",\n",
    "          \"Captivating story and great visuals.\",\n",
    "          \"Confusing boring childish.\",\n",
    "          \"Strong Hero. Weak Film.\"]\n",
    "\n",
    "get_sentiment(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Topic modeling\n",
    "Topic modeling is an unsupervised ML method used to find inherent structure in documents. It learns\n",
    "representations of topics in documents which allows grouping of different documents together. We will\n",
    "use ```Gensim``` library and Latent Dirichlet Allocation (LDA) for this.\n",
    "\n",
    "LDA’s approach to topic modeling is it considers each document as a collection of topics in a certain proportion. And each topic as a collection of keywords, again, in a certain proportion.\n",
    "\n",
    "Once you provide the algorithm with the number of topics, all it does it to rearrange the topics distribution within the documents and keywords distribution within the topics to obtain a good composition of topic-keywords distribution.\n",
    "\n",
    "Adapted from https://kleiber.me/blog/2017/07/22/tutorial-lda-wikipedia/\n",
    "\n",
    "![Topic modeling](https://i.stack.imgur.com/vI8Lc.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.921902Z",
     "start_time": "2021-11-10T21:26:59.921884Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import wikipedia, random\n",
    "\n",
    "def fetch_data(article_names):\n",
    "    '''Fetching the data from given Wikipedia articles'''\n",
    "    wikipedia_random_articles = wikipedia.random(2)\n",
    "    wikipedia_random_articles.extend(article_names)\n",
    "    wikipedia_random_articles\n",
    "    print(wikipedia_random_articles)\n",
    "    \n",
    "    wikipedia_articles = []\n",
    "    for wikipedia_article in wikipedia_random_articles:\n",
    "        wikipedia_articles.append([wikipedia_article, \n",
    "                                   wikipedia.page(wikipedia_article).content])\n",
    "    return wikipedia_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.923512Z",
     "start_time": "2021-11-10T21:26:59.923492Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')    \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def clean(article):\n",
    "    '''Cleaning the article contents and getting the word stems'''\n",
    "    title, document = article\n",
    "    tokens = RegexpTokenizer(r'\\w+').tokenize(document.lower())\n",
    "    tokens_clean = [token for token in tokens if token not in \n",
    "                    stopwords.words('english')]\n",
    "    tokens_stemmed = [PorterStemmer().stem(token) for token \n",
    "                      in tokens_clean]\n",
    "    return (title, tokens_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.925962Z",
     "start_time": "2021-11-10T21:26:59.925938Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "article_names = ['Music', 'Grace Hopper', 'Portland, Oregon', 'Data', 'Compiler', 'Oregon']\n",
    "wikipedia_articles = fetch_data(article_names)\n",
    "wikipedia_articles\n",
    "wikipedia_articles_clean = list(map(clean, wikipedia_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.927759Z",
     "start_time": "2021-11-10T21:26:59.927698Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "article_contents = [article[1] for article in wikipedia_articles_clean]\n",
    "dictionary = corpora.Dictionary(article_contents)\n",
    "corpus = [dictionary.doc2bow(article) for article in \n",
    "          article_contents[:-1]] # All except 'Compiler'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.929736Z",
     "start_time": "2021-11-10T21:26:59.929712Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=6, \n",
    "                                            id2word = dictionary, \n",
    "                                            passes=100)\n",
    "\n",
    "topic_results = lda_model.print_topics(num_topics=6, num_words=5)\n",
    "topic_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.931677Z",
     "start_time": "2021-11-10T21:26:59.931651Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.933612Z",
     "start_time": "2021-11-10T21:26:59.933527Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(list(lda_model[[dictionary.doc2bow(article_contents[-1])]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Resources\n",
    "\n",
    "- More on NLP https://monkeylearn.com/blog/definitive-guide-natural-language-processing/\n",
    "- A very comprehensive list of resources by Penn https://www.seas.upenn.edu/~romap/nlp-resources.html\n",
    "- Peter Norvig's spell corrector http://norvig.com/spell-correct.html\n",
    "- Applications and datasets https://machinelearningmastery.com/datasets-natural-language-processing/\n",
    "- More datasets https://gengo.ai/datasets/the-best-25-datasets-for-natural-language-processing/\n",
    "- https://towardsdatascience.com/text-analytics-topic-modelling-on-music-genres-song-lyrics-deb82c86caa2\n",
    "- Collection of tutorials https://medium.com/machine-learning-in-practice/over-200-of-the-best-machine-learning-nlp-and-python-tutorials-2018-edition-dd8cf53cb7dc\n",
    "- Text classification https://textminingonline.com/dive-into-nltk-part-vii-a-preliminary-study-on-text-classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unsupervised NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.935768Z",
     "start_time": "2021-11-10T21:26:59.935747Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.938793Z",
     "start_time": "2021-11-10T21:26:59.938768Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "winedf = pd.read_csv('winemag-data_first150k.csv')\n",
    "winedf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.941843Z",
     "start_time": "2021-11-10T21:26:59.941824Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "winedf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.943629Z",
     "start_time": "2021-11-10T21:26:59.943612Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "winedf[winedf['description'].duplicated(keep = False)].sort_values('description').head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.945913Z",
     "start_time": "2021-11-10T21:26:59.945891Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "winedf = winedf.drop_duplicates('description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.947341Z",
     "start_time": "2021-11-10T21:26:59.947312Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "variety_df = winedf.groupby('variety').filter(lambda x: len(x) > 1500)\n",
    "varieties = variety_df['variety'].value_counts().index.tolist()\n",
    "fig, ax = plt.subplots(figsize = (25, 10))\n",
    "sns.countplot(x = variety_df['variety'], order = varieties, ax = ax)\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.951295Z",
     "start_time": "2021-11-10T21:26:59.951270Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "variety_df['variety'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.953162Z",
     "start_time": "2021-11-10T21:26:59.953137Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "punc = ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',\"%\"]\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(punc)\n",
    "desc = variety_df['description'].values\n",
    "vectorizer = TfidfVectorizer(stop_words = stop_words)\n",
    "X = vectorizer.fit_transform(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.955602Z",
     "start_time": "2021-11-10T21:26:59.955576Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "word_features = vectorizer.get_feature_names()\n",
    "word_features[550:575]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.957562Z",
     "start_time": "2021-11-10T21:26:59.957542Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z\\']+')\n",
    "\n",
    "def tokenize(text):\n",
    "    return [stemmer.stem(word) for word in tokenizer.tokenize(text.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.960197Z",
     "start_time": "2021-11-10T21:26:59.960177Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer2 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize)\n",
    "X2 = vectorizer2.fit_transform(desc)\n",
    "word_features2 = vectorizer2.get_feature_names()\n",
    "word_features2[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.962342Z",
     "start_time": "2021-11-10T21:26:59.962317Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer3 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize, max_features = 1000)\n",
    "X3 = vectorizer3.fit_transform(desc)\n",
    "words = vectorizer3.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.963872Z",
     "start_time": "2021-11-10T21:26:59.963848Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.966281Z",
     "start_time": "2021-11-10T21:26:59.966256Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 15, n_init = 5, n_jobs = -1)\n",
    "kmeans.fit(X3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.968498Z",
     "start_time": "2021-11-10T21:26:59.968472Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "common_words = kmeans.cluster_centers_.argsort()[:,-1:-11:-1]\n",
    "for num, centroid in enumerate(common_words):\n",
    "    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.971314Z",
     "start_time": "2021-11-10T21:26:59.971258Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "variety_df['cluster'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.972762Z",
     "start_time": "2021-11-10T21:26:59.972739Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "variety_df['cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T21:26:59.975950Z",
     "start_time": "2021-11-10T21:26:59.975925Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "clusters = variety_df.groupby(['cluster', 'variety']).size()\n",
    "fig2, ax2 = plt.subplots(figsize = (30, 15))\n",
    "sns.heatmap(clusters.unstack(level = 'variety'), ax = ax2, cmap = 'Reds')\n",
    "\n",
    "ax2.set_xlabel('variety', fontdict = {'weight': 'bold', 'size': 24})\n",
    "ax2.set_ylabel('cluster', fontdict = {'weight': 'bold', 'size': 24})\n",
    "for label in ax2.get_xticklabels():\n",
    "    label.set_size(16)\n",
    "    label.set_weight(\"bold\")\n",
    "for label in ax2.get_yticklabels():\n",
    "    label.set_size(16)\n",
    "    label.set_weight(\"bold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Books**\n",
    "\n",
    "Here are a few helpful references:\n",
    "\n",
    "- [**Speech and Language Processing**](https://web.stanford.edu/~jurafsky/slp3/), by Dan Jurafsky and James H. Martin (free PDF)\n",
    "\n",
    "- [**Introduction to Information Retrieval**](https://nlp.stanford.edu/IR-book/html/htmledition/irbook.html) by By Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze (free online)\n",
    "\n",
    "- [**Natural Language Processing with PyTorch**](https://learning.oreilly.com/library/view/natural-language-processing/9781491978221/) by Brian McMahan and Delip Rao (need to purchase or have O'Reilly Safari account) \n",
    "\n",
    "**Blogs**\n",
    "\n",
    "Good NLP-related blogs:\n",
    "- [Sebastian Ruder](http://ruder.io/)\n",
    "- [Joyce Xu](https://medium.com/@joycex99)\n",
    "- [Jay Alammar](https://jalammar.github.io/)\n",
    "- [Stephen Merity](https://smerity.com/articles/articles.html)\n",
    "- [Rachael Tatman](https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
